{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Regression Learning:\n",
    "- Numerical prediction given examples of inputs and outputs trained with data\n",
    "- As opposed to classification learning\n",
    "\n",
    "Types of learning:\n",
    "- Linear regression (parametric)\n",
    "- K nearest neighbor (instance based)\n",
    "- Decision trees\n",
    "- Decision forests (lots and lots of decision trees taken together)\n",
    "\n",
    "Learning with stock data:\n",
    "- given a dataframe with features of a set of stocks over time (measureable predictive factors)\n",
    "- feature data serves as the input X\n",
    "- for training, use historical prices and historical features for learning\n",
    "- use a historical price in the future from a feature and record price(t+5) vs. feature set(t) pairs as data to train the model\n",
    "- depth & breadth given by the time period over which training occurs and the stock universe we will look at\n",
    "- once model is trained, start doing predictions\n",
    "\n",
    "Problems with regression:\n",
    "- noisy and uncertain\n",
    "- challenging to estimate confidence in forecasts\n",
    "- holding time, allocation uncertain\n",
    "- can be addressed by enforcement learning \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Supervised Regression Learning\n",
    "- Using data to build a model that predicts a numerical outputs based on a set of numerical inputs.\n",
    "\n",
    "#### Parametric regression: Simple Regression\n",
    "- represent the model with a number of parameters\n",
    "- for example, fitting a line to data (through linear regression on the line y = mx + b)\n",
    "    - parameters given by m and b\n",
    "- could fit, theoretically, any polynomial with additional parameters to try and describe the behavior of the data more accurately\n",
    "- in practice, much of the time you throw away the data once the model is parameterized and use that for predictions\n",
    "\n",
    "#### Instance regression: K Nearest Neighbor\n",
    "- instead, you could use a data-centric approach where you keep the data and use it to better inform you predictions\n",
    "- find K nearest data points to the query and use them to estimate the output prediction\n",
    "- take the mean Y value of the K nearest neighbors for the prediction\n",
    "- if you repeat this process you would have a model that fits the data more appropriately\n",
    "- another similar method is kernel regression, that assigns a weight to each neighbor based on the distance from the query X value (or cartesian distance)\n",
    "- non-parametric approaches are good for models that are hard to approximate/derive mathematically, and instead are well-suited for numerical methods instead\n",
    "\n",
    "#### Training and Testing:\n",
    "- we have data on prices and features for our stocks\n",
    "- we first want to separate testing and training data, to be able to see if the model behaves well once the model has been trained appropriately\n",
    "- take training data, put it through machine learning model to derive the parameters, then use testing data and put it through the model, and compare the output to the true prices that we know to see if the model has been successful\n",
    "- generally, train on older data and test on newer data \n",
    "\n",
    "#### Learning APIs:\n",
    "- will need to build api's for implementing the learners\n",
    "\n",
    "##### Linear Regression:\n",
    "- learner = LinRegLearner()\n",
    "- learner.train(Xtrain,Ytrain)\n",
    "- Y = learner.query(Xtest) --> compare to Ytest\n",
    "\n",
    "<code> class LinRegLearner::\n",
    "    def __init__(self): \n",
    "        pass\n",
    "        \n",
    "    def train(self,X,Y):\n",
    "        # fit a line to the data\n",
    "        # find an m and a b --> parameters of linear model\n",
    "        self.m, self.b = favorite_linreg(X,Y) # use algo you want from SciPy and Numpy\n",
    "        \n",
    "    def query(self,X):\n",
    "        Y = self.m*X + self.b\n",
    "        return Y\n",
    "</code>\n",
    "\n",
    "##### K-Nearest Neighbor:\n",
    "- learner = KNNLearner(k=3) --> arg = number of neighbors\n",
    "- learner.train(Xtrain,Ytrain)\n",
    "- Y = learner.query(Xtest) --> compare to Ytest\n",
    "\n",
    "<code> class KNNLearner::\n",
    "    def __init__(self,k): \n",
    "        self.k = k\n",
    "        pass\n",
    "        \n",
    "    def train(self,X,Y):\n",
    "        # find set of Y values given k for each value of X\n",
    "        # don't really have to train much\n",
    "        \n",
    "    def query(self,X):\n",
    "        Y = average Y-value of k-nearest neighbors\n",
    "        return Y\n",
    "</code>\n",
    "    \n",
    "#####\n",
    "\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
